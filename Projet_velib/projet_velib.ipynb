{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##       Recommandation des stations velib dans le 1er arrondissement de Paris "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.8:4041\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1575219770182)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5ce7d301\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: String = 2.4.4\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "import org.apache.spark.sql.functions.udf\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.DataFrameNaFunctions\n",
       "import org.apache.spark.sql.types._\n",
       "import scala.util.parsing.json._\n",
       "import scala.concurrent.Future\n",
       "import scala.concurrent.ExecutionContext.Implicits.global\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.classification.{LogisticRegression, DecisionTreeClassifier}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.ap..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions.udf\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.DataFrameNaFunctions\n",
    "import org.apache.spark.sql.types._\n",
    "import scala.util.parsing.json._\n",
    "//import slick.driver.PostgresDriver.api._\n",
    "import scala.concurrent.Future\n",
    "import scala.concurrent.ExecutionContext.Implicits.global\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, DecisionTreeClassifier}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.feature.{OneHotEncoderEstimator}\n",
    "import org.apache.spark.ml.stat.Summarizer\n",
    "import org.apache.spark.ml.linalg.{Matrix, Vectors}\n",
    "import org.apache.spark.ml.stat.Correlation\n",
    "import org.apache.spark.sql.Row\n",
    "import scala.sys.process._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Données vélib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "velib: org.apache.spark.sql.DataFrame = [Nombre de bornes disponibles: int, Nombre vélo en PARK+1: int ... 19 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val velib: DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", true) // utilise la première ligne du (des) fichier(s) comme header\n",
    "      .option(\"inferSchema\", \"true\") // pour inférer le type de chaque colonne (Int, String, etc.)\n",
    "      .option(\"delimiter\", \";\")\n",
    "      .csv(\"data/velib-dispo-2810-matin.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes : 1390\n",
      "Nombre de colonnes : 21\n"
     ]
    }
   ],
   "source": [
    "println(s\"Nombre de lignes : ${velib.count}\")\n",
    "println(s\"Nombre de colonnes : ${velib.columns.length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nombre de bornes disponibles: integer (nullable = true)\n",
      " |-- Nombre vélo en PARK+1: integer (nullable = true)\n",
      " |-- Nombres de bornes en station: integer (nullable = true)\n",
      " |-- PARK + activation: string (nullable = true)\n",
      " |-- densityLevel: integer (nullable = true)\n",
      " |-- Achat possible en station (CB): string (nullable = true)\n",
      " |-- Description station: string (nullable = true)\n",
      " |-- maxBikeOverflow: integer (nullable = true)\n",
      " |-- Etat du Totem: string (nullable = true)\n",
      " |-- nbFreeDock: integer (nullable = true)\n",
      " |-- Nombre de vélo mécanique: integer (nullable = true)\n",
      " |-- PARK +: string (nullable = true)\n",
      " |-- nbDock: integer (nullable = true)\n",
      " |-- Nombre vélo électrique: integer (nullable = true)\n",
      " |-- Nombre vélo en PARK+14: integer (nullable = true)\n",
      " |-- Code de la station: integer (nullable = true)\n",
      " |-- Nom de la station: string (nullable = true)\n",
      " |-- Etat des stations: string (nullable = true)\n",
      " |-- Type de stations: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- duedate: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "velib.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adresses: org.apache.spark.sql.DataFrame = [id: string, id_fantoir: string ... 18 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val adresses: DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", true) // utilise la première ligne du (des) fichier(s) comme header\n",
    "      .option(\"inferSchema\", \"true\") // pour inférer le type de chaque colonne (Int, String, etc.)\n",
    "      .option(\"delimiter\", \";\")\n",
    "      .csv(\"data/adresses-75.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes : 151430\n",
      "Nombre de colonnes : 20\n"
     ]
    }
   ],
   "source": [
    "println(s\"Nombre de lignes : ${adresses.count}\")\n",
    "println(s\"Nombre de colonnes : ${adresses.columns.length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- id_fantoir: string (nullable = true)\n",
      " |-- numero: integer (nullable = true)\n",
      " |-- rep: string (nullable = true)\n",
      " |-- nom_voie: string (nullable = true)\n",
      " |-- code_postal: integer (nullable = true)\n",
      " |-- code_insee: integer (nullable = true)\n",
      " |-- nom_commune: string (nullable = true)\n",
      " |-- code_insee_ancienne_commune: string (nullable = true)\n",
      " |-- nom_ancienne_commune: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- alias: string (nullable = true)\n",
      " |-- nom_ld: string (nullable = true)\n",
      " |-- libelle_acheminement: string (nullable = true)\n",
      " |-- nom_afnor: string (nullable = true)\n",
      " |-- source_position: string (nullable = true)\n",
      " |-- source_nom_voie: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adresses.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Cleaning data velib et adresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "velibClean_1: org.apache.spark.sql.DataFrame = [nbre_bornes_dispo: int, nbre_bornes_station: int ... 12 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val velibClean_1: DataFrame =  velib\n",
    "    .withColumn(\"x_velib\", split($\"geo\", \",\").getItem(1).cast(\"double\"))\n",
    "    .withColumn(\"y_velib\", split($\"geo\", \",\").getItem(0).cast(\"double\"))\n",
    "    .withColumnRenamed(\"Nombre de bornes disponibles\",\"nbre_bornes_dispo\")\n",
    "    .withColumnRenamed(\"Nombres de bornes en station\",\"nbre_bornes_station\")\n",
    "    .withColumnRenamed(\"Achat possible en station (CB)\",\"achat_cb\")\n",
    "    .withColumnRenamed(\"Description station\",\"description\")\n",
    "    .withColumnRenamed(\"Etat du Totem\",\"etat_totem\")\n",
    "    .withColumnRenamed(\"Nombre de vélo mécanique\",\"nbre_velos_meca\")\n",
    "    .withColumnRenamed(\"Nombre vélo électrique\",\"nbre_velos_elec\")\n",
    "    .withColumnRenamed(\"Code de la station\",\"cod_station\")\n",
    "    .withColumnRenamed(\"Nom de la station\",\"nom_station\")\n",
    "    .withColumnRenamed(\"Etat des stations\",\"etat_station\")\n",
    "    .withColumnRenamed(\"Type de stations\",\"type_station\")\n",
    "    .drop($\"Nombre vélo en PARK+1\")\n",
    "    .drop($\"PARK + activation\")\n",
    "    .drop($\"Nombre vélo en PARK+14\")\n",
    "    .drop($\"PARK +\")\n",
    "    .drop($\"densityLevel\")\n",
    "    .drop($\"description\")\n",
    "    .drop($\"maxBikeOverflow\")\n",
    "    .drop($\"nbFreeDock\")\n",
    "    .drop($\"nbDock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nbre_bornes_dispo: integer (nullable = true)\n",
      " |-- nbre_bornes_station: integer (nullable = true)\n",
      " |-- achat_cb: string (nullable = true)\n",
      " |-- etat_totem: string (nullable = true)\n",
      " |-- nbre_velos_meca: integer (nullable = true)\n",
      " |-- nbre_velos_elec: integer (nullable = true)\n",
      " |-- cod_station: integer (nullable = true)\n",
      " |-- nom_station: string (nullable = true)\n",
      " |-- etat_station: string (nullable = true)\n",
      " |-- type_station: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- duedate: timestamp (nullable = true)\n",
      " |-- x_velib: double (nullable = true)\n",
      " |-- y_velib: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "velibClean_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url: String = https://opendata.paris.fr/api/records/1.0/search/?dataset=velib-disponibilite-en-temps-reel&facet=records&rows=1391\n",
       "data: String = {\"nhits\": 1392, \"parameters\": {\"dataset\": \"velib-disponibilite-en-temps-reel\", \"timezone\": \"UTC\", \"rows\": 1391, \"format\": \"json\", \"facet\": [\"records\"]}, \"records\": [{\"datasetid\": \"velib-disponibilite-en-temps-reel\", \"recordid\": \"f46ded6cfc239a5912224badc4f014fa9c13c534\", \"fields\": {\"nbfreeedock\": 25, \"station_state\": \"Operative\", \"maxbikeoverflow\": 0, \"creditcard\": \"no\", \"station_type\": \"yes\", \"overflowactivation\": \"no\", \"station_code\": \"16107\", \"overflow\": \"no\", \"nbbikeoverflow\": 0, \"duedate\": \"2018-07-15\", \"densitylevel\": \"1\", \"nbedock\": 35, \"station\": \"{\\\"code\\\": \\\"16107\\\", \\\"name\\\": \\\"Benjamin Godard - Victor Hugo\\\", \\\"state\\\": \\\"Operative\\..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val url = \"https://opendata.paris.fr/api/records/1.0/search/?dataset=velib-disponibilite-en-temps-reel&facet=overflowactivation&facet=creditcard&facet=kioskstate&facet=station_state\" \n",
    "val url = \"https://opendata.paris.fr/api/records/1.0/search/?dataset=velib-disponibilite-en-temps-reel&facet=records&rows=1391\"\n",
    "val data = scala.io.Source.fromURL(url).mkString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dft: org.apache.spark.sql.DataFrame = [nhits: bigint, parameters: struct<dataset: string, facet: array<string> ... 3 more fields> ... 1 more field]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dft = spark.read.json(Seq(data).toDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newDF: org.apache.spark.sql.DataFrame = [datasetid: string, fields: struct<creditcard: string, densitylevel: string ... 19 more fields> ... 3 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newDF = dft.select(explode($\"records\").as(\"app\"))\n",
    "            .select(\"app.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataFinal: org.apache.spark.sql.DataFrame = [station_name: string, station_code: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataFinal = newDF.select(\"fields.station_name\", \"fields.station_code\", \"fields.nbfreeedock\", \"fields.nbedock\", \"fields.nbfreedock\", \"fields.nbbike\", \"fields.nbdock\", \"fields.nbebike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newNames: Seq[String] = List(nom_station, cod_station, nbre_bornes_dispo_elec, nbre_bornes_station_elec, nbre_bornes_dispo, nbre_velos_meca, nbre_bornes_station, nbre_velos_elec)\n",
       "dataFinalRenamed: org.apache.spark.sql.DataFrame = [nom_station: string, cod_station: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newNames = Seq(\"nom_station\", \"cod_station\", \"nbre_bornes_dispo_elec\", \"nbre_bornes_station_elec\", \"nbre_bornes_dispo\", \"nbre_velos_meca\", \"nbre_bornes_station\", \"nbre_velos_elec\")\n",
    "val dataFinalRenamed = dataFinal.toDF(newNames: _*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "velibClean: org.apache.spark.sql.DataFrame = [cod_station: string, nom_station: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val velibClean = dataFinalRenamed\n",
    "    .select($\"nom_station\", $\"cod_station\", $\"nbre_bornes_dispo_elec\", $\"nbre_bornes_station_elec\", $\"nbre_bornes_dispo\", $\"nbre_velos_meca\", $\"nbre_bornes_station\", $\"nbre_velos_elec\")\n",
    "    .join(velibClean_1.select($\"cod_station\", $\"x_velib\", $\"y_velib\"), \"cod_station\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cod_station: string (nullable = true)\n",
      " |-- nom_station: string (nullable = true)\n",
      " |-- nbre_bornes_dispo_elec: long (nullable = true)\n",
      " |-- nbre_bornes_station_elec: long (nullable = true)\n",
      " |-- nbre_bornes_dispo: long (nullable = true)\n",
      " |-- nbre_velos_meca: long (nullable = true)\n",
      " |-- nbre_bornes_station: long (nullable = true)\n",
      " |-- nbre_velos_elec: long (nullable = true)\n",
      " |-- x_velib: double (nullable = true)\n",
      " |-- y_velib: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "velibClean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Récupérationndes adresses du premier arrondissement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adresses01: org.apache.spark.sql.DataFrame = [id: string, id_fantoir: string ... 18 more fields]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val adresses01: DataFrame = adresses.filter($\"code_postal\" === 75001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adresseClean: org.apache.spark.sql.DataFrame = [id: string, id_fantoir: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val adresseClean: DataFrame =  adresses01\n",
    "    .drop($\"code_insee_ancienne_commune\")\n",
    "    .drop($\"lon\")\n",
    "    .drop($\"lat\")\n",
    "    .drop($\"alias\")\n",
    "    .drop($\"nom_ld\")\n",
    "    .drop($\"libelle_acheminement\")\n",
    "    .drop($\"nom_afnor\")\n",
    "    .drop($\"source_position\")\n",
    "    .drop($\"source_nom_voie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- id_fantoir: string (nullable = true)\n",
      " |-- numero: integer (nullable = true)\n",
      " |-- rep: string (nullable = true)\n",
      " |-- nom_voie: string (nullable = true)\n",
      " |-- code_postal: integer (nullable = true)\n",
      " |-- code_insee: integer (nullable = true)\n",
      " |-- nom_commune: string (nullable = true)\n",
      " |-- nom_ancienne_commune: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adresseClean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Recommandation de la station la plus proche de chaque adresse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossJoin: org.apache.spark.sql.DataFrame = [id: string, id_fantoir: string ... 19 more fields]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val crossJoin = adresseClean.crossJoin(velibClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+-----------+-------------+\n",
      "|numero|x        |y         |x_velib    |y_velib      |\n",
      "+------+---------+----------+-----------+-------------+\n",
      "|1     |650852.89|6862571.29|646867.5476|6863127.31356|\n",
      "|2     |650873.97|6862544.35|646867.5476|6863127.31356|\n",
      "|3     |650864.23|6862595.32|646867.5476|6863127.31356|\n",
      "|4     |650884.79|6862565.18|646867.5476|6863127.31356|\n",
      "|5     |650876.02|6862619.38|646867.5476|6863127.31356|\n",
      "|6     |650912.95|6862621.07|646867.5476|6863127.31356|\n",
      "|7     |650887.5 |6862643.14|646867.5476|6863127.31356|\n",
      "|8     |650963.6 |6862727.6 |646867.5476|6863127.31356|\n",
      "|9     |650899.37|6862666.99|646867.5476|6863127.31356|\n",
      "|10    |650992.43|6862786.23|646867.5476|6863127.31356|\n",
      "|11    |650920.92|6862710.52|646867.5476|6863127.31356|\n",
      "|13    |650930.61|6862730.5 |646867.5476|6863127.31356|\n",
      "|15    |650940.52|6862750.6 |646867.5476|6863127.31356|\n",
      "|17    |650949.37|6862771.29|646867.5476|6863127.31356|\n",
      "|19    |650958.59|6862791.73|646867.5476|6863127.31356|\n",
      "|21    |650968.29|6862811.6 |646867.5476|6863127.31356|\n",
      "|22    |651005.11|6862809.96|646867.5476|6863127.31356|\n",
      "|1     |650560.95|6863090.61|646867.5476|6863127.31356|\n",
      "|2     |650575.62|6863088.7 |646867.5476|6863127.31356|\n",
      "|3     |650569.27|6863107.18|646867.5476|6863127.31356|\n",
      "+------+---------+----------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crossJoin.select($\"numero\", $\"x\", $\"y\",$\"x_velib\", $\"y_velib\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossJoinDistInt: org.apache.spark.sql.DataFrame = [id: string, id_fantoir: string ... 21 more fields]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val crossJoinDistInt: DataFrame = crossJoin\n",
    "    .withColumn(\"dist_km\", round(sqrt(pow($\"x_velib\" - $\"x\", 2) + pow($\"y_velib\" - $\"y\", 2))/1000, 3))\n",
    "    .withColumn(\"proba\",($\"nbre_velos_meca\"/ $\"nbre_bornes_station\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossJoinDist: org.apache.spark.sql.DataFrame = [id: string, id_fantoir: string ... 22 more fields]\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val crossJoinDist: DataFrame = crossJoinDistInt\n",
    "    .withColumn(\"esperance\", round(pow($\"dist_km\" + 1, -1) * $\"proba\" ,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inter: org.apache.spark.sql.DataFrame = [id: string, dist_min: double]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inter: DataFrame = crossJoinDist.groupBy(\"id\").agg(min($\"dist_km\").as(\"dist_min\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StationProche: org.apache.spark.sql.DataFrame = [id: string, dist_min: double ... 24 more fields]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val StationProche: DataFrame = inter.join(crossJoinDist, (crossJoinDist.col(\"id\") ===  inter.col(\"id\") && crossJoinDist.col(\"dist_km\") === inter.col(\"dist_min\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation de quelques recommandations par la distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------+-------------------------------------+-----------+---------------+-------+\n",
      "|numero|nom_voie                            |nom_station                          |cod_station|nbre_velos_meca|dist_km|\n",
      "+------+------------------------------------+-------------------------------------+-----------+---------------+-------+\n",
      "|163   |Rue Saint-honore                    |Saint-Honoré - Musée du Louvre       |1023       |10             |0.003  |\n",
      "|4     |Rue de Ventadour                    |Ventadour - Opéra                    |1116       |5              |0.004  |\n",
      "|2     |Rue d'Alger                         |Alger - Rivoli                       |1018       |13             |0.004  |\n",
      "|19    |Rue des Halles                      |Halles - Bourdonnais                 |1021       |12             |0.005  |\n",
      "|6     |Rue Francaise                       |Française - Etienne Marcel           |1102       |8              |0.006  |\n",
      "|217   |Rue Saint-honore                    |Saint-Honoré - 29 juillet            |1017       |10             |0.006  |\n",
      "|25    |Rue des Lavandieres Sainte-opportune|Lavandieres Sainte Opportune - Rivoli|1120       |8              |0.006  |\n",
      "|4     |Rue de la Grande Truanderie         |Grande Truanderie - Saint-Denis      |1006       |10             |0.006  |\n",
      "|23    |Rue des Lavandieres Sainte-opportune|Lavandieres Sainte Opportune - Rivoli|1120       |8              |0.007  |\n",
      "|188   |Rue Saint-honore                    |Place du Palais Royal                |1013       |16             |0.007  |\n",
      "+------+------------------------------------+-------------------------------------+-----------+---------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "StationProche.select($\"numero\", $\"nom_voie\", $\"nom_station\", $\"cod_station\", $\"nbre_velos_meca\", $\"dist_km\")\n",
    "    .distinct().sort($\"dist_km\".asc).show(10,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Recommandation de la station ayant la plus forte espérance (Espérance basée sur la distance à la station et la probabilité d'avoir un vélo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "esperances: org.apache.spark.sql.DataFrame = [id: string, esperance_max: double]\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val esperances: DataFrame = crossJoinDist.groupBy(\"id\").agg(max($\"esperance\").as(\"esperance_max\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stationForteEsp: org.apache.spark.sql.DataFrame = [id: string, esperance_max: double ... 24 more fields]\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stationForteEsp: DataFrame = esperances.join(crossJoinDist, (crossJoinDist.col(\"id\") ===  esperances.col(\"id\") && crossJoinDist.col(\"esperance\") === esperances.col(\"esperance_max\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation de quelques recommandations par l'espérance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+---------------------------------+-----------+---------------+-------+---------+\n",
      "|numero|nom_voie          |nom_station                      |cod_station|nbre_velos_meca|dist_km|esperance|\n",
      "+------+------------------+---------------------------------+-----------+---------------+-------+---------+\n",
      "|1     |Bd de la Madeleine|Gare Saint-Lazare - Cour du Havre|8002       |26             |0.671  |15.56    |\n",
      "|5     |Pl Henri Salvador |Gare Saint-Lazare - Cour du Havre|8002       |26             |0.673  |15.541   |\n",
      "|3     |Bd de la Madeleine|Gare Saint-Lazare - Cour du Havre|8002       |26             |0.673  |15.541   |\n",
      "|5     |Bd de la Madeleine|Gare Saint-Lazare - Cour du Havre|8002       |26             |0.677  |15.504   |\n",
      "|39    |Rue des Capucines |Gare Saint-Lazare - Cour du Havre|8002       |26             |0.68   |15.476   |\n",
      "|7     |Bd de la Madeleine|Gare Saint-Lazare - Cour du Havre|8002       |26             |0.68   |15.476   |\n",
      "|53    |Rue Cambon        |Gare Saint-Lazare - Cour du Havre|8002       |26             |0.681  |15.467   |\n",
      "|3     |Pl Henri Salvador |Gare Saint-Lazare - Cour du Havre|8002       |26             |0.681  |15.467   |\n",
      "|9     |Bd de la Madeleine|Gare Saint-Lazare - Cour du Havre|8002       |26             |0.683  |15.449   |\n",
      "|23    |Rue des Capucines |Gare Saint-Lazare - Cour du Havre|8002       |26             |0.687  |15.412   |\n",
      "+------+------------------+---------------------------------+-----------+---------------+-------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stationForteEsp.select($\"numero\", $\"nom_voie\", $\"nom_station\", $\"cod_station\", $\"nbre_velos_meca\", $\"dist_km\", $\"esperance\")\n",
    "    .distinct().sort($\"esperance\".desc).show(10,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Essai de mise en place d'un modèle de prédiction de la disponibilité des vélos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement des données vélib des 5 dernières semaines et  rééchantillonnage des données toutes les 15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "velib_43: org.apache.spark.sql.DataFrame = [station_id: int, available_bikes: int ... 4 more fields]\n",
       "velib_44: org.apache.spark.sql.DataFrame = [station_id: int, available_bikes: int ... 4 more fields]\n",
       "velib_45: org.apache.spark.sql.DataFrame = [station_id: int, available_bikes: int ... 4 more fields]\n",
       "velib_46: org.apache.spark.sql.DataFrame = [station_id: int, available_bikes: int ... 4 more fields]\n",
       "velib_47: org.apache.spark.sql.DataFrame = [station_id: int, available_bikes: int ... 4 more fields]\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val velib_43: DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", true) // utilise la première ligne du (des) fichier(s) comme header\n",
    "      .option(\"inferSchema\", \"true\") // pour inférer le type de chaque colonne (Int, String, etc.)\n",
    "      .option(\"delimiter\", \",\")\n",
    "      .csv(\"data/velib_43.csv\")\n",
    "\n",
    " val velib_44: DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", true) // utilise la première ligne du (des) fichier(s) comme header\n",
    "      .option(\"inferSchema\", \"true\") // pour inférer le type de chaque colonne (Int, String, etc.)\n",
    "      .option(\"delimiter\", \",\")\n",
    "      .csv(\"data/velib_44.csv\")\n",
    "\n",
    " val velib_45: DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", true) // utilise la première ligne du (des) fichier(s) comme header\n",
    "      .option(\"inferSchema\", \"true\") // pour inférer le type de chaque colonne (Int, String, etc.)\n",
    "      .option(\"delimiter\", \",\")\n",
    "      .csv(\"data/velib_45.csv\")\n",
    "\n",
    " val velib_46: DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", true) // utilise la première ligne du (des) fichier(s) comme header\n",
    "      .option(\"inferSchema\", \"true\") // pour inférer le type de chaque colonne (Int, String, etc.)\n",
    "      .option(\"delimiter\", \",\")\n",
    "      .csv(\"data/velib_46.csv\")\n",
    "\n",
    " val velib_47: DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", true) // utilise la première ligne du (des) fichier(s) comme header\n",
    "      .option(\"inferSchema\", \"true\") // pour inférer le type de chaque colonne (Int, String, etc.)\n",
    "      .option(\"delimiter\", \",\")\n",
    "      .csv(\"data/velib_47.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fusion de ces données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "velib_union: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [station_id: int, available_bikes: int ... 4 more fields]\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val velib_union = velib_43.union(velib_44).union(velib_45).union(velib_46).union(velib_47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes : 14025261\n",
      "Nombre de colonnes : 6\n"
     ]
    }
   ],
   "source": [
    "println(s\"Nombre de lignes : ${velib_union.count}\")\n",
    "println(s\"Nombre de colonnes : ${velib_union.columns.length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- available_bikes: integer (nullable = true)\n",
      " |-- available_ebikes: integer (nullable = true)\n",
      " |-- free_stands: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- updated: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "velib_union.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+----------------+-----------+---------+----------+\n",
      "|station_id|available_bikes|available_ebikes|free_stands|status   |updated   |\n",
      "+----------+---------------+----------------+-----------+---------+----------+\n",
      "|4020      |2              |4               |17         |Operative|1571608811|\n",
      "|4021      |3              |1               |22         |Operative|1571608811|\n",
      "|32303     |4              |3               |23         |Operative|1571608811|\n",
      "|32302     |3              |6               |11         |Operative|1571608811|\n",
      "|32301     |1              |10              |6          |Operative|1571608811|\n",
      "+----------+---------------+----------------+-----------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "velib_union.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajout des colonnes relatives au temps (date, jour, heure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "velib_union_add: org.apache.spark.sql.DataFrame = [station_id: int, available_bikes: int ... 9 more fields]\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val velib_union_add = velib_union\n",
    "    .withColumn(\"date_column\",from_unixtime($\"updated\"))\n",
    "    .withColumn(\"day_week\",dayofweek(from_unixtime($\"updated\")))\n",
    "    .withColumn(\"week_year\",weekofyear(from_unixtime($\"updated\")))\n",
    "    .withColumn(\"hour\", hour(from_unixtime($\"updated\")))\n",
    "    .withColumn(\"minute\", minute(from_unixtime($\"updated\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sélection des données dans la tranche horaire 6h-19h et réechantillonnage toutes les 15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "velib_union_filter: org.apache.spark.sql.DataFrame = [station_id: int, available_bikes: int ... 9 more fields]\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val velib_union_filter: DataFrame = velib_union_add.filter(($\"hour\" >= 6 && $\"hour\" <= 19) && ($\"minute\" === 0 || $\"minute\" === 15 || $\"minute\" === 30 || $\"minute\" === 45) && ($\"status\" === \"Operative\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- available_bikes: integer (nullable = true)\n",
      " |-- available_ebikes: integer (nullable = true)\n",
      " |-- free_stands: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- updated: integer (nullable = true)\n",
      " |-- date_column: string (nullable = true)\n",
      " |-- day_week: integer (nullable = true)\n",
      " |-- week_year: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "velib_union_filter.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Long = 2160818\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "velib_union_filter.select().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "velib_union_final: org.apache.spark.sql.DataFrame = [station_id: int, available_bikes: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val velib_union_final :DataFrame = velib_union_filter\n",
    "    .join(velibClean.select($\"cod_station\", $\"x_velib\", $\"y_velib\"), velib_union_filter.col(\"station_id\").equalTo(velibClean(\"cod_station\")))\n",
    "    .drop(\"cod_station\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+----------------+-----------+---------+----------+-------------------+--------+---------+----+------+-------------+-------------+\n",
      "|station_id|available_bikes|available_ebikes|free_stands|   status|   updated|        date_column|day_week|week_year|hour|minute|      x_velib|      y_velib|\n",
      "+----------+---------------+----------------+-----------+---------+----------+-------------------+--------+---------+----+------+-------------+-------------+\n",
      "|      4101|             12|               4|          7|Operative|1571630417|2019-10-21 06:00:17|       2|       43|   6|     0|653667.671224|6861717.91514|\n",
      "|      4101|             12|               4|          7|Operative|1571631312|2019-10-21 06:15:12|       2|       43|   6|    15|653667.671224|6861717.91514|\n",
      "|      4101|             12|               4|          7|Operative|1571632211|2019-10-21 06:30:11|       2|       43|   6|    30|653667.671224|6861717.91514|\n",
      "|      4101|             12|               4|          7|Operative|1571633121|2019-10-21 06:45:21|       2|       43|   6|    45|653667.671224|6861717.91514|\n",
      "|      4101|             12|               4|          7|Operative|1571634023|2019-10-21 07:00:23|       2|       43|   7|     0|653667.671224|6861717.91514|\n",
      "+----------+---------------+----------------+-----------+---------+----------+-------------------+--------+---------+----+------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "velib_union_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essai de détermination de modèle de prédiction sur une seule station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "velib_union_inter: org.apache.spark.sql.DataFrame = [station_id: int, available_bikes: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val velib_union_inter :DataFrame = velib_union_final.filter($\"station_id\" === 4020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+----------------+-----------+---------+----------+-------------------+--------+---------+----+------+-------------+-------------+\n",
      "|station_id|available_bikes|available_ebikes|free_stands|   status|   updated|        date_column|day_week|week_year|hour|minute|      x_velib|      y_velib|\n",
      "+----------+---------------+----------------+-----------+---------+----------+-------------------+--------+---------+----+------+-------------+-------------+\n",
      "|      4020|              2|               2|         19|Operative|1571630417|2019-10-21 06:00:17|       2|       43|   6|     0|652315.514663|6862429.59699|\n",
      "|      4020|              2|               2|         19|Operative|1571631312|2019-10-21 06:15:12|       2|       43|   6|    15|652315.514663|6862429.59699|\n",
      "|      4020|              2|               2|         19|Operative|1571632210|2019-10-21 06:30:10|       2|       43|   6|    30|652315.514663|6862429.59699|\n",
      "|      4020|              2|               2|         19|Operative|1571633121|2019-10-21 06:45:21|       2|       43|   6|    45|652315.514663|6862429.59699|\n",
      "|      4020|              2|               2|         19|Operative|1571634023|2019-10-21 07:00:23|       2|       43|   7|     0|652315.514663|6862429.59699|\n",
      "+----------+---------------+----------------+-----------+---------+----------+-------------------+--------+---------+----+------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "velib_union_inter.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Econdage du jour de la semaine et l'heure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_6f0e13a6981b\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val encoder = new OneHotEncoderEstimator()\n",
    "      .setInputCols(Array(\"day_week\", \"hour\", \"minute\"))\n",
    "      .setOutputCols(Array(\"day_week_onehot\", \"hour_onehot\", \"minute_onehot\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mise des données sous forme interprétable par Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_7a53cc01332e\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Mettre les données sous une forme utilisable par Spark.ML\n",
    "val assembler = new VectorAssembler()\n",
    "        .setInputCols(Array(\"day_week_onehot\", \"hour_onehot\", \"minute_onehot\"))\n",
    "        .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application de la logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_dc8af231b85d\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Stage 10 : créer/instancier le modèle de classification\n",
    " val lr = new LogisticRegression()\n",
    "       .setFeaturesCol(\"features\")\n",
    "       .setLabelCol(\"available_bikes\")\n",
    "       .setPredictionCol(\"predictions\")\n",
    "       .setRawPredictionCol(\"raw_predictions\")\n",
    "       .setMaxIter(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application de la DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_aa04091ee04a\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val dt = new DecisionTreeClassifier()\n",
    "       .setFeaturesCol(\"features\")\n",
    "       .setLabelCol(\"available_bikes\")\n",
    "       .setPredictionCol(\"predictions\")\n",
    "       .setRawPredictionCol(\"raw_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création pipeline et Split des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_88041325ea51\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline = new Pipeline()\n",
    "        .setStages(Array(encoder, assembler, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [station_id: int, available_bikes: int ... 11 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [station_id: int, available_bikes: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Entraînement, test, et sauvegarde du modèle\n",
    "val Array(training, test) = velib_union_inter.randomSplit(Array(0.9, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_23b48c504eee\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Entraînement du modèle\n",
    "val model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_dt: org.apache.spark.ml.Pipeline = pipeline_addaef0efbc1\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_dt = new Pipeline()\n",
    "        .setStages(Array(encoder, assembler, dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_dt: org.apache.spark.ml.PipelineModel = pipeline_addaef0efbc1\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Entraînement du modèle\n",
    "val model_dt = pipeline_dt.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation de la prédiction  avec la logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.7705882352941177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "simplePredictions: org.apache.spark.sql.DataFrame = [station_id: int, available_bikes: int ... 18 more fields]\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_8760c6976521\n",
       "accuracy: Double = 0.22941176470588234\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val simplePredictions = model.transform(test)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"available_bikes\")\n",
    "  .setPredictionCol(\"predictions\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "\n",
    "val accuracy = evaluator.evaluate(simplePredictions)\n",
    "println(\"Test Error = \" + (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'erreur obtenue suite avec le modèle **Logistic Regression** est très élevé. Afin de voir si cela est dû à nos variables explicatives, nous allons essayer un autre modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation de la prédiction  avec le classifieur DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.8224852071005917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "simplePredictionsDt: org.apache.spark.sql.DataFrame = [station_id: int, available_bikes: int ... 18 more fields]\n",
       "evaluatorDt: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_4c6b25e5cf32\n",
       "accuracy: Double = 0.17751479289940827\n"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val simplePredictionsDt = model_dt.transform(test)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluatorDt = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"available_bikes\")\n",
    "  .setPredictionCol(\"predictions\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "\n",
    "val accuracy = evaluatorDt.evaluate(simplePredictionsDt)\n",
    "println(\"Test Error = \" + (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que la précision avec le classifieur **Decision Tree** est aussi médiocre que celle de la **Logistic Regression**.\n",
    "\n",
    "- On peut donc affirmer que les variables explicatives choisies ne permettent pas de prédire le nombre de vélos disponibles en station.\n",
    "\n",
    "- Afin d'améliorer notre prédiction, nous allons intégrer les paramètres météorologiques (température, humidité...) dans nos variables explicatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des data météo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "meteo: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 10 more fields]\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val meteo: DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", true) // utilise la première ligne du (des) fichier(s) comme header\n",
    "      .option(\"inferSchema\", \"true\") // pour inférer le type de chaque colonne (Int, String, etc.)\n",
    "      .option(\"delimiter\", \";\")\n",
    "      .csv(\"data/meteo_15_29.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Minute: integer (nullable = true)\n",
      " |-- Temperature  [2 m above gnd]: double (nullable = true)\n",
      " |-- Relative Humidity  [2 m above gnd]: double (nullable = true)\n",
      " |-- Total Precipitation (high resolution)  [sfc]: double (nullable = true)\n",
      " |-- Sunshine Duration  [sfc]: double (nullable = true)\n",
      " |-- Shortwave Radiation  [sfc]: double (nullable = true)\n",
      " |-- Wind Speed  [10 m above gnd]: double (nullable = true)\n",
      " |-- Wind Direction  [10 m above gnd]: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meteo.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nettoyage des données météo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "meteoClean_1: org.apache.spark.sql.DataFrame = [Month: int, Day: int ... 7 more fields]\n",
       "meteoClean: org.apache.spark.sql.DataFrame = [month: int, day_meteo: int ... 7 more fields]\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val meteoClean_1: DataFrame = meteo.select( $\"Month\",$\"Day\",$\"Hour\",$\"Minute\", $\"Temperature  [2 m above gnd]\", $\"Relative Humidity  [2 m above gnd]\",$\"Total Precipitation (high resolution)  [sfc]\", $\"Sunshine Duration  [sfc]\", $\"Wind Speed  [10 m above gnd]\" )\n",
    "\n",
    "val meteoClean: DataFrame = meteoClean_1\n",
    "      .withColumnRenamed(\"Month\", \"month\")\n",
    "      .withColumnRenamed(\"Day\", \"day_meteo\")\n",
    "      .withColumnRenamed(\"Hour\", \"hour_meteo\")\n",
    "      .withColumnRenamed(\"Minute\", \"minute_meteo\")\n",
    "      .withColumnRenamed(\"Temperature  [2 m above gnd]\", \"temperature\")\n",
    "      .withColumnRenamed(\"Relative Humidity  [2 m above gnd]\", \"humidite\")\n",
    "      .withColumnRenamed(\"Total Precipitation (high resolution)  [sfc]\", \"precipitation\")\n",
    "      .withColumnRenamed(\"Sunshine Duration  [sfc]\", \"ensol\")\n",
    "      .withColumnRenamed(\"Wind Speed  [10 m above gnd]\", \"wind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day_meteo: integer (nullable = true)\n",
      " |-- hour_meteo: integer (nullable = true)\n",
      " |-- minute_meteo: integer (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidite: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- ensol: double (nullable = true)\n",
      " |-- wind: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meteoClean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "velib_15_29: org.apache.spark.sql.DataFrame = [station_id: int, available_bikes: int ... 12 more fields]\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val velib_15_29 :DataFrame =  velib_union_inter.filter((month($\"date_column\") === 11)  && (dayofmonth($\"date_column\")>= 15 && dayofmonth($\"date_column\") <= 29))\n",
    "                         .withColumn(\"day_month\",dayofmonth($\"date_column\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- available_bikes: integer (nullable = true)\n",
      " |-- available_ebikes: integer (nullable = true)\n",
      " |-- free_stands: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- updated: integer (nullable = true)\n",
      " |-- date_column: string (nullable = true)\n",
      " |-- day_week: integer (nullable = true)\n",
      " |-- week_year: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      " |-- x_velib: double (nullable = true)\n",
      " |-- y_velib: double (nullable = true)\n",
      " |-- day_month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "velib_15_29.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fusion des données météo et vélib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfMeteoVelib: org.apache.spark.sql.DataFrame = [station_id: int, available_bikes: int ... 21 more fields]\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfMeteoVelib = velib_15_29.join(meteoClean, velib_15_29.col(\"day_month\").equalTo(meteoClean(\"day_meteo\")) &&  velib_15_29.col(\"hour\").equalTo(meteoClean(\"hour_meteo\")) && velib_15_29.col(\"minute\").equalTo(meteoClean(\"minute_meteo\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder_meteo: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_1eedbeb6c6d4\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val encoder_meteo = new OneHotEncoderEstimator()\n",
    "      .setInputCols(Array( \"day_week\", \"hour\", \"minute\"))\n",
    "      .setOutputCols(Array(\"day_week_onehot\", \"hour_onehot\", \"minute_onehot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler_time: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_f36723d2b0ef\n",
       "assembler_meteo: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_4c1a6b25b392\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler_time =  new VectorAssembler()\n",
    "        .setInputCols(Array(\"day_week_onehot\", \"hour_onehot\", \"minute_onehot\"))\n",
    "        .setOutputCol(\"time\")\n",
    "\n",
    "\n",
    "val assembler_meteo = new VectorAssembler()\n",
    "        .setInputCols(Array(\"temperature\", \"time\"))\n",
    "        .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du pipeline de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_meteo: org.apache.spark.ml.Pipeline = pipeline_04f76ba3dd8e\n",
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [station_id: int, available_bikes: int ... 21 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [station_id: int, available_bikes: int ... 21 more fields]\n",
       "model_meteo: org.apache.spark.ml.PipelineModel = pipeline_04f76ba3dd8e\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_meteo = new Pipeline()\n",
    "        .setStages(Array(encoder_meteo, assembler_time, assembler_meteo, lr))\n",
    "\n",
    "//Entraînement, test, et sauvegarde du modèle\n",
    "val Array(training, test) = dfMeteoVelib.randomSplit(Array(0.9, 0.1))\n",
    "\n",
    "// Entraînement du modèle\n",
    "val model_meteo = pipeline_meteo.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.7857142857142857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "simplePredictionsMeteo: org.apache.spark.sql.DataFrame = [station_id: int, available_bikes: int ... 29 more fields]\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_9f3d5454190b\n",
       "accuracy: Double = 0.21428571428571427\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val simplePredictionsMeteo = model_meteo.transform(test)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"available_bikes\")\n",
    "  .setPredictionCol(\"predictions\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "\n",
    "val accuracy = evaluator.evaluate(simplePredictionsMeteo)\n",
    "println(\"Test Error = \" + (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On constate que l'introduction des données météo n'améliore pas la précision de notre modèle.** Il faudrait d'une part essayer de travailler sur des données sur une plus grande période (plusieurs années) et d'autre part trouver d'autres variables explicatives permettant d'expliquer le nombre de vélos disponible dans une station (proximité,  d'un monument touristique, de bureaux, fonctionnement des autres transports...).\n",
    "\n",
    "Il se pourrait aussi que les algorithmes utilisés ne soient pas adaptés à notre étude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-----------------------------+\n",
      "|predictions|available_bikes|features                     |\n",
      "+-----------+---------------+-----------------------------+\n",
      "|4.0        |3              |(27,[0,7,21],[3.18,1.0,1.0]) |\n",
      "|3.0        |4              |(27,[0,3,17],[6.13,1.0,1.0]) |\n",
      "|4.0        |6              |(27,[0,5,16],[0.29,1.0,1.0]) |\n",
      "|6.0        |11             |(27,[0,4],[7.31,1.0])        |\n",
      "|11.0       |13             |(27,[0,3],[7.58,1.0])        |\n",
      "|12.0       |13             |(27,[0,5,24],[8.01,1.0,1.0]) |\n",
      "|7.0        |14             |(27,[0,6,22],[10.59,1.0,1.0])|\n",
      "|14.0       |13             |(27,[0,22],[11.7,1.0])       |\n",
      "+-----------+---------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simplePredictionsMeteo.select(\"predictions\", \"available_bikes\", \"features\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
